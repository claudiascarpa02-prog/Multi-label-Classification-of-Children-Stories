{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YdCt7WSuOoGd"
   },
   "source": [
    "This notebook serves as a starting point for the final project of the Introduction to Machine Learning 2024/25 course. You must use exactly the code provided to load and split the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GtsSBn9AfKXE"
   },
   "source": [
    "# Code to load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5233,
     "status": "ok",
     "timestamp": 1752664779173,
     "user": {
      "displayName": "Claudia Scarpa",
      "userId": "07942809746367462788"
     },
     "user_tz": -120
    },
    "id": "_2LCie5Mac7i",
    "outputId": "f582479f-e1d4-4d4d-c9b6-95258951f17c"
   },
   "outputs": [],
   "source": [
    "!pip install -U \"datasets==3.6.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21203,
     "status": "ok",
     "timestamp": 1752664800401,
     "user": {
      "displayName": "Claudia Scarpa",
      "userId": "07942809746367462788"
     },
     "user_tz": -120
    },
    "id": "ErtziEODYM6U",
    "outputId": "b14ac4bd-2aca-4521-b975-6af01700b8ee"
   },
   "outputs": [],
   "source": [
    "### YOU MUST NOT CHANGE THIS CELL! ###\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "full_dataset = load_dataset(\"skeskinen/TinyStories-GPT4\", split=\"train\")\n",
    "full_dataset = full_dataset.remove_columns([c for c in full_dataset.column_names if c not in [\"story\", \"features\"]])\n",
    "assert len(full_dataset) == 2745100\n",
    "\n",
    "splits = full_dataset.train_test_split(test_size=10000, seed=42, shuffle=True)\n",
    "\n",
    "train_dataset = splits[\"train\"]\n",
    "test_dataset  = splits[\"test\"]\n",
    "\n",
    "assert len(train_dataset) == 2735100\n",
    "assert len(test_dataset)  == 10000\n",
    "\n",
    "assert train_dataset[0][\"story\"][:33] == \"One day, a little girl named Lily\"\n",
    "assert train_dataset[0][\"features\"] == [\"Dialogue\", \"Conflict\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 47,
     "status": "ok",
     "timestamp": 1752664800407,
     "user": {
      "displayName": "Claudia Scarpa",
      "userId": "07942809746367462788"
     },
     "user_tz": -120
    },
    "id": "G__WK0ycbIvh",
    "outputId": "4dd59d27-f3bf-408e-8486-fddc89d83def"
   },
   "outputs": [],
   "source": [
    "# Here we print the first example of the train dataset\n",
    "\n",
    "from pprint import pprint\n",
    "pprint(train_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hZbsWoo8Utbx"
   },
   "source": [
    "\n",
    "\n",
    "# MODEL 2 : Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qscHujZ9WcrX"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import copy\n",
    "import pandas as pd\n",
    "import hashlib\n",
    "import json\n",
    "import os\n",
    "import string\n",
    "\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_recall_curve, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split, ParameterGrid\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rJb-UUT2rP3I"
   },
   "outputs": [],
   "source": [
    "# Fix the seed for the reproducibility\n",
    "SEED = 42\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6pESm0o30yvY"
   },
   "outputs": [],
   "source": [
    "# Define the device:\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RFENBPHXWBgs"
   },
   "outputs": [],
   "source": [
    "TAGS = [\"BadEnding\", \"Conflict\", \"Dialogue\", \"Foreshadowing\", \"MoralValue\", \"Twist\"]\n",
    "\n",
    "def f1_per_tag(true_tag_lists, pred_tag_lists):\n",
    "\n",
    "    scores = f1_score(true_tag_lists, pred_tag_lists, average=None, zero_division=0)\n",
    "    return {t: float(s) for t, s in zip(TAGS, scores)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wFUbKlLP1s_J"
   },
   "outputs": [],
   "source": [
    "num_labels = len(TAGS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mu9-DZlF8X5_"
   },
   "source": [
    "Create a smaller training dataset, representative of all the labels:\n",
    "\n",
    "Since we are unable to train the model on the entire dataset of stories, we want to create a smaller training set that is representative of all the labels. To ensure that no tag is significantly underrepresented, we impose a minimum number of stories for each label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 224214,
     "status": "ok",
     "timestamp": 1752665024656,
     "user": {
      "displayName": "Claudia Scarpa",
      "userId": "07942809746367462788"
     },
     "user_tz": -120
    },
    "id": "s-k1Gv45WD9H",
    "outputId": "8b2850ea-5bec-439b-eae2-a26595c1f1a6"
   },
   "outputs": [],
   "source": [
    "# Construction of a small training dataset\n",
    "target_size = 400000      # Dimension of the small dataset\n",
    "min_per_tag = 8000       # Minimum number of stories per tag\n",
    "\n",
    "\n",
    "# Map each label to the list of indices of the stories that contain it\n",
    "tag_to_indices = defaultdict(list) # Create a dictionary that maps each tag to an empty list.\n",
    "for idx, example in enumerate(train_dataset): # Iterate over each example in the train_dataset, retrieving the index idx of the current example.\n",
    "    for tag in example['features']:\n",
    "        if tag in TAGS:\n",
    "            tag_to_indices[tag].append(idx) # If the tag is one of the tags of interest, it adds the index idx of that story to the list corresponding to that tag in the dictionary.\n",
    "\n",
    "# To avoid duplicated stories\n",
    "final_indices = set()\n",
    "for tag in TAGS:\n",
    "    indices_for_tag = tag_to_indices[tag] # Retrieves the list of story indices containing that tag from the previously created tag_to_indices dictionary.\n",
    "\n",
    "    num_to_sample = min(min_per_tag, len(indices_for_tag))  # If there are less stories with that tag than min_per_tag we take them all\n",
    "    sampled_indices = random.sample(indices_for_tag, num_to_sample) # Randomly samples num_to_sample indices from the indices_for_tag list.\n",
    "    final_indices.update(sampled_indices) # Adds the sampled indices to the final_indices set.\n",
    "\n",
    "print(f\"Unique stories after guaranteeing minimums: {len(final_indices)}\")\n",
    "\n",
    "# Add random stories to reach target_size.\n",
    "num_to_add = target_size - len(final_indices) # How many stories left to reach target_size\n",
    "if num_to_add > 0:\n",
    "    # Identify all the indices not already selected\n",
    "    all_indices = set(range(len(train_dataset)))\n",
    "    remaining_indices = list(all_indices - final_indices)\n",
    "    random.shuffle(remaining_indices) # Shuffle the remaining indices\n",
    "\n",
    "    final_indices.update(remaining_indices[:num_to_add]) # Add the first num_to_add indices of the shuffled list\n",
    "\n",
    "# Create the final dataset from the selected indices\n",
    "final_indices_list = list(final_indices) # Convert it to a list otherwise it cannot be shuffled\n",
    "\n",
    "random.shuffle(final_indices_list) # To avoid having an ordered dataset\n",
    "\n",
    "# To ensure the final dimension to be target_size\n",
    "if len(final_indices_list) > target_size:\n",
    "    final_indices_list = final_indices_list[:target_size]\n",
    "\n",
    "# Reduced 'controlled' dataset\n",
    "small_dataset = train_dataset.select(final_indices_list)\n",
    "\n",
    "print(f\"Final 'small_dataset' created with {len(small_dataset)} stories.\")\n",
    "\n",
    "# Verify the new label distribution\n",
    "print(\"\\nVerifying the new label distribution:\")\n",
    "new_counts = {tag: 0 for tag in TAGS}\n",
    "for item_features in small_dataset['features']:\n",
    "    for feature in item_features:\n",
    "        if feature in new_counts:\n",
    "            new_counts[feature] += 1\n",
    "pprint(new_counts)\n",
    "\n",
    "# Extract stories and labels from the small dataset\n",
    "stories, labels = small_dataset['story'], small_dataset['features']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3xnI-bbSgS-A"
   },
   "outputs": [],
   "source": [
    "# # DOWNLOAD SMALL DATASET\n",
    "# import csv\n",
    "# from google.colab import files\n",
    "\n",
    "# df_small = pd.DataFrame({\n",
    "#     'stories': stories,\n",
    "#     'labels': [str(l) for l in labels]  # This mantains the commas\n",
    "# })\n",
    "# # Create a DataFrame and I convert elements in strings\n",
    "\n",
    "# df_small.to_csv('small_dataset_400.000.csv', index=False, quoting=1)  # Save the DataFrame in a csv file\n",
    "# files.download('small_dataset_400.000.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5eKcrPHYPZV0"
   },
   "source": [
    "Convert every label to a binary vector:\n",
    "\n",
    "We define a mapping from each label to a unique index, then create a function to convert lists of labels into binary vectors. This allows us to represent the presence or absence of each label as 1s and 0s for model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cAUcqamo-r09"
   },
   "outputs": [],
   "source": [
    "label_to_index = {label: i for i, label in enumerate(TAGS)} # Map each label to a unique index (BadEnding->0, Conflict->1,...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "me2ZVxN_W9Lk"
   },
   "outputs": [],
   "source": [
    "def list_to_binary(label_list):\n",
    "    vector = [0] * len(TAGS) # Initialize a binary vector of zeros with length equal to number of labels\n",
    "    for label in label_list:\n",
    "        label = label.strip() # Remove any extra whitespace around the label\n",
    "        vector[label_to_index[label]] = 1 # Set the position corresponding to this label to 1\n",
    "    return vector\n",
    "\n",
    "def binary_to_list(vector):\n",
    "  return [TAGS[i] for i, value in enumerate(vector) if value == 1]\n",
    "\n",
    "labels_bin = [list_to_binary(label) for label in labels] # Convert all labels into binary vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lk3SxWlQsmXh"
   },
   "outputs": [],
   "source": [
    "# Define a function that separate the words from the punctuation with an empty space and then split the stories into tokens:\n",
    "def simple_tokenizer(stories):\n",
    "\n",
    "    punctuation = string.punctuation    # Construct a string for the punctuation\n",
    "    all_tokens = []\n",
    "\n",
    "    for text in stories:\n",
    "        # Convert text to lowercase\n",
    "        text = text.lower()\n",
    "\n",
    "        # Iterate over each punctuation character\n",
    "        for char in punctuation:\n",
    "            # Add a space before and after that character in the text\n",
    "            text = text.replace(char, f' {char} ')\n",
    "\n",
    "        # Split the tokens when there is a space\n",
    "        tokens = text.split()\n",
    "        all_tokens.append(tokens)\n",
    "\n",
    "    return all_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4gEp4_AmIy3V"
   },
   "outputs": [],
   "source": [
    "# Apply the simple_tokenizer function to all the stories of the small_dataset:\n",
    "stories_split = simple_tokenizer(stories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MWzxgqX5JBLD"
   },
   "source": [
    "Create the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GTdupcmuJCQ8"
   },
   "outputs": [],
   "source": [
    "# Define a special token for padding and for unknown tokens\n",
    "PAD_TOKEN = '[PAD]'\n",
    "UNK_TOKEN = '[UNK]'\n",
    "\n",
    "# Flatten the list of tokenized stories to get a single list of all tokens\n",
    "all_tokens = [token for story in stories_split for token in story]\n",
    "\n",
    "# Remove duplicates and sort the tokens\n",
    "vocab_tokens = sorted(set(all_tokens))\n",
    "\n",
    "V = [PAD_TOKEN, UNK_TOKEN] + vocab_tokens\n",
    "token_to_index = {token: idx for idx, token in enumerate(V)}\n",
    "\n",
    "vocab_size = len(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lfA9qtEpe20D"
   },
   "outputs": [],
   "source": [
    "# Define a function to encode a story:\n",
    "def encode(x):\n",
    "    return [token_to_index[token] if token in token_to_index else token_to_index[UNK_TOKEN] for token in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 127,
     "status": "ok",
     "timestamp": 1752665047222,
     "user": {
      "displayName": "Claudia Scarpa",
      "userId": "07942809746367462788"
     },
     "user_tz": -120
    },
    "id": "wwynaxQQ_WLi",
    "outputId": "ef54cfca-c9c7-4fbf-df73-42c202847099"
   },
   "outputs": [],
   "source": [
    "# How to choose max_length\n",
    "\n",
    "lengths = [len(story) for story in stories_split]              # Compute the length of eash story\n",
    "max_length = int(np.percentile(lengths, 95))                    # Fix max_length s.t. 95% of the stories are shorter than max_length\n",
    "\n",
    "print(\"Max length chosen:\", max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RD53XJM0zlNl"
   },
   "outputs": [],
   "source": [
    "# Define a function to have all the stories of the same length:\n",
    "def truncate_and_pad(sequence):\n",
    "    sequence = copy.copy(sequence)\n",
    "    if len(sequence) > max_length:      # If the story is too long, take only the last max_length tokens => We give more weight to the end of the story rather than the beginning\n",
    "        sequence = sequence[-max_length:]\n",
    "    elif len(sequence) < max_length:    # If the story is too short add some PAD tokens (at the end of the story)\n",
    "        sequence = sequence + [token_to_index[PAD_TOKEN]] * (max_length - len(sequence))\n",
    "\n",
    "    return sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5lemRPsRn-Ab"
   },
   "outputs": [],
   "source": [
    "# Apply the encode function to all the stories in small_dataset\n",
    "stories_encoded = [encode(story) for story in stories_split]\n",
    "\n",
    "# Apply truncate_and_pad function to each story\n",
    "stories_padded = [truncate_and_pad(story) for story in stories_encoded]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cpAfNKsCJWtT"
   },
   "source": [
    "Split the 'controlled' smaller dataset into training and validation sets, keeping 80% for training and 20% for validation, with a fixed random seed for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QbL0Ugg4glIw"
   },
   "outputs": [],
   "source": [
    "# Divide the small_dataset in train_dataset and val_dataset\n",
    "train_stories, val_stories, train_labels, val_labels = train_test_split(stories_padded, labels_bin, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eFCgJ5USt2q7"
   },
   "source": [
    "Create the DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dpIqMgaB3rWQ"
   },
   "outputs": [],
   "source": [
    "# Construct the TensorDataset\n",
    "train_dataset =TensorDataset(torch.tensor(train_stories, dtype = torch.long), torch.tensor(train_labels, dtype = torch.float))\n",
    "val_dataset = TensorDataset(torch.tensor(val_stories, dtype = torch.long), torch.tensor(val_labels, dtype = torch.float))\n",
    "\n",
    "# Construct DataLoader instances\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lhsS2Klv9Nio"
   },
   "source": [
    "Define the training and evaluating procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8D8QH4a5zoPc"
   },
   "outputs": [],
   "source": [
    "# Training procedure:\n",
    "def train(model, train_loader, val_loader, optimizer, epochs, pos_weights, device, save_best_model=False):\n",
    "    model.to(device)    # Move the model to the correct device\n",
    "\n",
    "    scheduler = StepLR(optimizer, step_size=round(epochs * 3/4), gamma=0.1, verbose=True)\n",
    "    loss_fn = nn.BCEWithLogitsLoss(pos_weight=pos_weights)   # Loss function with the pos_weights\n",
    "\n",
    "    # Initialize\n",
    "    best_f1_scores_per_tag = np.zeros(len(TAGS))\n",
    "    pred_prob = []\n",
    "    pred_labels = []\n",
    "\n",
    "    best_true_labels = None   # Initialize a variable for the true labels associated to the best model\n",
    "\n",
    "\n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        model.train()   # Training mode\n",
    "        losses = []\n",
    "\n",
    "        # Iterate over the elements of the batch\n",
    "        for data, target in train_loader:\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            optimizer.zero_grad()   # Reset the gradients\n",
    "            output = model(data)    # Forward pass\n",
    "            loss = loss_fn(output, target)    # Compute the losses\n",
    "            loss.backward()   # Backward pass\n",
    "            optimizer.step()    # Update the weights\n",
    "\n",
    "            losses.append(loss.item())\n",
    "\n",
    "        print(f'Train epoch: {epoch + 1} | Loss: {np.mean(losses):.4f}')\n",
    "\n",
    "        # Evaluate after each epoch\n",
    "        current_avg_loss, current_f1_values, current_pred_prob, current_pred_labels, true_labels= evaluate(model, val_loader, device)\n",
    "\n",
    "        # Check for improved model (at least one F1 better, and the rest not below 0.75)\n",
    "        improvements = current_f1_values > best_f1_scores_per_tag\n",
    "        at_least_one_improved = improvements.any()\n",
    "\n",
    "        # Acceptable degradation (they must remain > 0.75)\n",
    "        non_improved = ~improvements\n",
    "        acceptable_degradation = np.all(current_f1_values[non_improved] >= 0.75) # boolean variable: True if all the tags have improved and they are > 0.75\n",
    "\n",
    "        if at_least_one_improved and acceptable_degradation:\n",
    "          best_f1_scores_per_tag = current_f1_values.copy()\n",
    "          pred_prob = current_pred_prob\n",
    "          pred_labels = current_pred_labels\n",
    "\n",
    "          # Save the true labels at the best epoch:\n",
    "          best_true_labels = true_labels.copy()       # Use .copy() to be sure\n",
    "\n",
    "          if save_best_model:\n",
    "            torch.save(model.state_dict(), \"best_model_CNN.pt\")\n",
    "\n",
    "          print(f\"✅ New best model saved (F1-based)\")\n",
    "\n",
    "        scheduler.step()\n",
    "        print(f\"Learning rate: {scheduler.get_last_lr()}\")\n",
    "\n",
    "    # If the model does not improve:\n",
    "    if best_true_labels is None:\n",
    "        best_true_labels = true_labels\n",
    "\n",
    "    return best_true_labels, pred_labels, pred_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2UeVOdm58G4n"
   },
   "outputs": [],
   "source": [
    "# Evaluate function\n",
    "def evaluate(model, loader, device):\n",
    "    model.to(device)\n",
    "    model.eval()    # Evaluation mode\n",
    "    loss = 0\n",
    "\n",
    "    loss_fn = nn.BCEWithLogitsLoss()\n",
    "    all_targets = []\n",
    "    all_outputs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in loader:\n",
    "            data = data.to(device)      # Access input_ids\n",
    "            target = target.to(device)  # Access labels\n",
    "\n",
    "            output = model(data)\n",
    "            loss += loss_fn(output, target).item()\n",
    "\n",
    "            all_targets.append(target.cpu().numpy())\n",
    "            all_outputs.append(output.cpu().numpy())\n",
    "\n",
    "\n",
    "    loss /= len(loader)\n",
    "\n",
    "    # Calculate F1 score per tag\n",
    "    all_targets = np.concatenate(all_targets)\n",
    "    all_outputs = np.concatenate(all_outputs)\n",
    "\n",
    "    # Apply sigmoid and threshold to get binary predictions\n",
    "    pred_probabilities = torch.sigmoid(torch.tensor(all_outputs)).numpy()\n",
    "    pred_tags = (pred_probabilities > 0.5).astype(int)\n",
    "\n",
    "    # Calculates F1 scores per label\n",
    "    f1_scores = f1_per_tag(all_targets, pred_tags)\n",
    "    current_f1_values = np.array([f1_scores[tag] for tag in TAGS]) # np.array with f1 values\n",
    "\n",
    "\n",
    "    print(f'Evaluation loss: {loss:.4f}')\n",
    "    print(\"F1 Score per tag:\")\n",
    "    for tag, score in f1_scores.items():\n",
    "        print(f\"  {tag}: {score:.4f}\")\n",
    "\n",
    "    return loss, current_f1_values, pred_probabilities, pred_tags, all_targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JeZxcegfDKjS"
   },
   "source": [
    "We add pos_weights to the loss function to solve the problem of class imbalance. Adding these weights ensures that the loss calculation accurately accounts for the distribution in each class.\n",
    "The weight for each tag is computed as the ratio between the number of negative examples over the number of positive examples. \\\\\n",
    "https://docs.pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html#torch.nn.BCEWithLogitsLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 463,
     "status": "ok",
     "timestamp": 1752665081668,
     "user": {
      "displayName": "Claudia Scarpa",
      "userId": "07942809746367462788"
     },
     "user_tz": -120
    },
    "id": "UySXFSmC4sLc",
    "outputId": "fa9b3183-8f5d-4f19-86a7-b27d266be48c"
   },
   "outputs": [],
   "source": [
    "# Transform labels_bin into a np.array:\n",
    "labels_bin_np = np.array(labels_bin)\n",
    "\n",
    "# Counts the number of positive occurences for each tag:\n",
    "num_positives = labels_bin_np.sum(axis=0)   # axis = 0 (sum for every tag)\n",
    "\n",
    "# Counts the number of negative occurences for each tag:\n",
    "num_negatives = labels_bin_np.shape[0] - num_positives    # labels_bin_np.shape[0] = number of rows = number of stories\n",
    "\n",
    "# Fix a control constant to prevent division by zero\n",
    "epsilon = 1e-6\n",
    "\n",
    "pos_weight_value = num_negatives / (num_positives + epsilon)\n",
    "\n",
    "# Transform pos_weights into a torch.tensor and pass it to the \"device\"\n",
    "pos_weight_tensor = torch.tensor(pos_weight_value, dtype=torch.float, device=device)\n",
    "\n",
    "print(\"\\n Pos_weight for each tag:\")\n",
    "for i, tag in enumerate(TAGS):\n",
    "    print(f\"- {tag}: {pos_weight_tensor[i].item():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NmyqMqPsOqCF"
   },
   "source": [
    "Function for the CNN model:\n",
    "In PyTorch, nn.Embedding() returns a $b \\times l \\times d$ tensor (where $b$ is the batch size, $l$ is the sequence length, and $d$ is the embedding dimension), whereas nn.Conv1d() espects a $b \\times d \\times l$ tensor. Therefore, if you use these layers in your 1-dimensional CNN, you need to swap the second and third\n",
    "dimension between the embedding layer and the first convolutional layer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "96B0sgS0Kujj"
   },
   "outputs": [],
   "source": [
    "class Transpose(nn.Module):\n",
    "    \"\"\"Swap two tensor dimensions inside a Sequential.\"\"\"\n",
    "    def __init__(self, dim0: int, dim1: int):\n",
    "        super().__init__()\n",
    "        self.dim0, self.dim1 = dim0, dim1\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x is returned as a **view**, so this is zero‑copy\n",
    "        return x.transpose(self.dim0, self.dim1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jLNwIRuxxaxd"
   },
   "source": [
    "### Grid Search\n",
    "\n",
    "We ran the following cells on a dataset of 50,000 stories to find the best hyperparameters.\n",
    "Theoretically, these hyperparameters should remain optimal even if we increase the size of the dataset, so we saved them manually and re-ran the entire script using only the model configured with the best parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_GYirQO7GFZu"
   },
   "outputs": [],
   "source": [
    "# def run_training_with_params(params, vocab_size, num_labels, epochs, pos_weight_tensor, train_loader, val_loader, device):\n",
    "#   # Parameters for the grid\n",
    "#   learning_rate = params[\"learning_rate\"]\n",
    "#   embedding_dim = params[\"embedding_dim\"]\n",
    "#   drop_out = params[\"dropout\"]\n",
    "#   filters_conv1 = params[\"filters_conv1\"]\n",
    "#   kernel_size_conv1 = params[\"kernel_size_conv1\"]\n",
    "#   filters_conv2 = params[\"filters_conv2\"]\n",
    "#   kernel_size_conv2 = params[\"kernel_size_conv2\"]\n",
    "\n",
    "#   # Build the model\n",
    "#   model = nn.Sequential(\n",
    "#       nn.Embedding(vocab_size, embedding_dim),\n",
    "#       Transpose(1, 2),\n",
    "#       nn.Conv1d(in_channels=embedding_dim, out_channels=filters_conv1, kernel_size=kernel_size_conv1, padding=\"same\"),\n",
    "#       nn.ReLU(),\n",
    "#       nn.MaxPool1d(kernel_size=2),\n",
    "#       nn.Conv1d(in_channels=filters_conv1, out_channels=filters_conv2, kernel_size=kernel_size_conv2, padding=\"same\"),\n",
    "#       nn.ReLU(),\n",
    "#       nn.AdaptiveMaxPool1d(1),\n",
    "#       nn.Flatten(),\n",
    "#       nn.Dropout(drop_out),\n",
    "#       nn.Linear(filters_conv2, num_labels)\n",
    "#       )\n",
    "#   model.to(device)\n",
    "\n",
    "#   # Optimizer:\n",
    "#   optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "#   # Training:\n",
    "#   y_true_bin, _ , y_pred_prob = train(\n",
    "#       model=model,\n",
    "#       optimizer=optimizer,\n",
    "#       train_loader=train_loader,\n",
    "#       val_loader=val_loader,\n",
    "#       epochs=epochs,\n",
    "#       pos_weights=pos_weight_tensor,\n",
    "#       device=device\n",
    "#       )\n",
    "\n",
    "#   # Predicted tags\n",
    "#   y_pred_bin = (y_pred_prob > 0.5).astype(int)\n",
    "\n",
    "#   # Macro-F1 score\n",
    "#   f1 = f1_score(y_true_bin, y_pred_bin, average='macro')\n",
    "\n",
    "#   return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yG-eotAo_kLk"
   },
   "outputs": [],
   "source": [
    "# epochs = 10\n",
    "\n",
    "# # Define the grid of hyperparameters\n",
    "# param_grid = {\n",
    "#     \"learning_rate\": [0.001, 0.0005],\n",
    "#     \"embedding_dim\" : [100, 200, 300],\n",
    "#     \"dropout\" : [0.4, 0.5],\n",
    "#     \"filters_conv1\": [64, 128],\n",
    "#     \"kernel_size_conv1\": [3, 5],\n",
    "#     \"filters_conv2\": [32, 64],\n",
    "#     \"kernel_size_conv2\": [3]\n",
    "# }\n",
    "\n",
    "# results = []\n",
    "\n",
    "# for params in ParameterGrid(param_grid):\n",
    "#     print(f\"\\n Training with parameters: {params}\\n\")\n",
    "\n",
    "#     f1 = run_training_with_params(params, vocab_size, num_labels, epochs, pos_weight_tensor, train_loader, val_loader, device)\n",
    "#     results.append({\"params\": params, \"f1\": f1})\n",
    "\n",
    "# # Find the best combination\n",
    "# best = max(results, key=lambda x: x[\"f1\"])\n",
    "# print(\"\\n--- Best hyperparameters found: ---\")\n",
    "# print(best[\"params\"])\n",
    "# print(f\"Macro-F1: {best['f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jlf84Dz-C3K9"
   },
   "outputs": [],
   "source": [
    "# # Converts results into a DataFrame:\n",
    "# df_results = pd.DataFrame([\n",
    "#     {**r[\"params\"], \"f1_score\": r[\"f1\"]} for r in results\n",
    "# ])\n",
    "\n",
    "# # Sorts by F1 in descending order:\n",
    "# df_results = df_results.sort_values(by=\"f1_score\", ascending=False)\n",
    "\n",
    "# print(df_results.to_string(index=False))\n",
    "\n",
    "# df_results.to_csv(\"grid_search_results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "juGJVBNtxhza"
   },
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1752665081747,
     "user": {
      "displayName": "Claudia Scarpa",
      "userId": "07942809746367462788"
     },
     "user_tz": -120
    },
    "id": "7IwlUqfmwW3j",
    "outputId": "c01274f2-ff63-485d-e916-0c2d05f995d1"
   },
   "outputs": [],
   "source": [
    "# Save the best hyperparameters: (by hand, but from the gridsearch)\n",
    "best_lr = 0.0005\n",
    "best_embedding_dim = 300\n",
    "best_drop_out = 0.4\n",
    "best_filters_conv1 = 128\n",
    "best_kernel_size_conv1 = 3\n",
    "best_filters_conv2 = 64\n",
    "best_kernel_size_conv2 = 3\n",
    "\n",
    "print (\"Best hyperparameters:\")\n",
    "print(best_lr, best_embedding_dim, best_drop_out, best_filters_conv1, best_kernel_size_conv1, best_filters_conv2, best_kernel_size_conv2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4201,
     "status": "ok",
     "timestamp": 1752665197595,
     "user": {
      "displayName": "Claudia Scarpa",
      "userId": "07942809746367462788"
     },
     "user_tz": -120
    },
    "id": "4muaSjvU_KGp",
    "outputId": "3eb153f4-1e88-4634-d732-169312518264"
   },
   "outputs": [],
   "source": [
    "!pip install torchinfo\n",
    "from torchinfo import summary\n",
    "\n",
    "# Definition of the model:\n",
    "model_A = nn.Sequential(\n",
    "    nn.Embedding(vocab_size, best_embedding_dim),\n",
    "    Transpose(1, 2),\n",
    "    nn.Conv1d(in_channels=best_embedding_dim, out_channels=best_filters_conv1, kernel_size=best_kernel_size_conv1, padding=\"same\"),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool1d(kernel_size=2),\n",
    "    nn.Conv1d(in_channels=best_filters_conv1, out_channels=best_filters_conv2, kernel_size=best_kernel_size_conv2, padding=\"same\"),\n",
    "    nn.ReLU(),\n",
    "    nn.AdaptiveMaxPool1d(1),\n",
    "    nn.Flatten(),\n",
    "    nn.Dropout(best_drop_out),\n",
    "    nn.Linear(best_filters_conv2, num_labels)\n",
    "    )\n",
    "\n",
    "dummy_input = torch.randint(0, vocab_size, (batch_size, max_length), dtype=torch.long)\n",
    "\n",
    "summary(model_A, input_data=dummy_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sFMrs7UpAaDR"
   },
   "outputs": [],
   "source": [
    "# Optimizer configuration:\n",
    "optimizer = torch.optim.Adam(model_A.parameters(), best_lr)\n",
    "\n",
    "# Training:\n",
    "epochs=10\n",
    "_, _, _ = train(model_A, train_loader, val_loader, optimizer, epochs, pos_weight_tensor, device, save_best_model=True)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "jLNwIRuxxaxd"
   ],
   "gpuType": "T4",
   "provenance": [
    {
     "file_id": "1pHxA8mn2YHMJi3eHhGIcFydFOqSmv-KW",
     "timestamp": 1751964177078
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
